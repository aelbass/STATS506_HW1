---
title: "HW6"
format: html
editor: visual
---

### Github URL: https://github.com/aelbass/STATS506_HW1.git

## Problem 1

```{r}
library(e1071)
library(Rcpp)

cppFunction("
double C_moment(NumericVector v, int k) {
  int n = v.size();
  double mean = 0.0;

  for (int i = 0; i < n; ++i) {
    mean += v[i];
  }
  mean /= n;

  // compute k-th central moment
  double total = 0.0;
  for (int i = 0; i < n; ++i) {
    total += pow(v[i] - mean, k);
  }

  return total / n;
}
")

set.seed(7)
x <- rnorm(1000)

C_moment(x, 2)
moment(x, order = 2, center = TRUE)

C_moment(x, 3)
moment(x, order = 3, center = TRUE)

C_moment(x, 4)
moment(x, order = 4, center = TRUE)
```

## Problem 2

### a. 

```{r}
#' Load waldCI class
source("waldCI.R")

#' bootstrapWaldCI class
#'
#' Extension of waldCI that stores data and bootstrap results.
#' @slot data The dataset to be resampled.
#' @slot fn A function applied to each bootstrap sample, returning a scalar.
#' @slot reps Number of bootstrap resamples.
#' @slot compute Either "serial" or "parallel".
#' @slot bootvals Numeric vector of bootstrap estimates.
#' @export
setClass("bootstrapWaldCI",
         slots = c(
           data = "ANY",
           fn = "function",
           reps = "numeric",
           compute = "character",
           bootvals = "numeric"
         ),
         contains = "waldCI")

.bootstrap_do <- function(fn, data, reps, compute = "serial") {

  n <- nrow(data)

  if (compute == "serial") {

    out <- replicate(reps, {
      idx <- sample(seq_len(n), n, replace = TRUE)
      fn(data[idx, , drop = FALSE])
    })

  } else if (compute == "parallel") {

    library(parallel)
    cl <- makeCluster(detectCores() - 1)
    clusterExport(cl, varlist = c("data", "fn"), envir = environment())

    out <- parSapply(cl, 1:reps, function(i) {
      idx <- sample(seq_len(n), n, replace = TRUE)
      fn(data[idx, , drop = FALSE])
    })
    stopCluster(cl)

  } else {
    stop("compute must be 'serial' or 'parallel'")
  }

  return(out)
}

#' Construct a bootstrapWaldCI object
#'
#' @param fn A function that takes a data set and returns a scalar.
#' @param data The data frame to bootstrap.
#' @param reps Number of bootstrap replicates.
#' @param level CI confidence level.
#' @param compute "serial" or "parallel".
#'
#' @return A bootstrapWaldCI object.
#' @export
makeBootstrapCI <- function(fn, data, reps = 1000,
                            level = 0.95,
                            compute = "serial") {

  stopifnot(is.function(fn))
  stopifnot(is.numeric(reps))

  bootvals <- .bootstrap_do(fn, data, reps, compute)

  m <- mean(bootvals)
  s <- sd(bootvals)

  # Create waldCI first
  parent_ci <- makeCI(level = level, mean = m, sterr = s)

  new("bootstrapWaldCI",
      level = parent_ci@level,
      mean = parent_ci@mean,
      sterr = parent_ci@sterr,
      data = data,
      fn = fn,
      reps = reps,
      compute = compute,
      bootvals = bootvals)
}

#' Recompute bootstrap for a bootstrapWaldCI object
#'
#' @param object A bootstrapWaldCI object
#' @return Updated bootstrapWaldCI with new bootstrap samples
#' @export
setGeneric("rebootstrap", function(object) {
  standardGeneric("rebootstrap")
})

#' @export
setMethod("rebootstrap", "bootstrapWaldCI",
          function(object) {

            newvals <- .bootstrap_do(object@fn,
                                     object@data,
                                     object@reps,
                                     object@compute)

            m <- mean(newvals)
            s <- sd(newvals)

            parent_ci <- makeCI(level = object@level,
                                mean = m,
                                sterr = s)

            object@mean <- parent_ci@mean
            object@sterr <- parent_ci@sterr
            object@bootvals <- newvals

            validObject(object)
            return(object)
          })
```


### b. 
```{r}
ci1 <- makeBootstrapCI(function(x) mean(x$y),
                       ggplot2::diamonds,
                       reps = 1000)
ci1
rebootstrap(ci1)

system.time(
  ci_serial <- makeBootstrapCI(function(x) mean(x$y),
                  ggplot2::diamonds,
                  reps = 5000,
                  compute = "serial")
)

system.time(
  ci_parallel <- makeBootstrapCI(function(x) mean(x$y),
                  ggplot2::diamonds,
                  reps = 5000,
                  compute = "parallel")
)

rebootstrap(ci_serial)
rebootstrap(ci_parallel)
```

In timing experiments with 5000 bootstrap replications on the diamonds data set, the serial computation was roughly twice as fast as the parallel execution. The parallel advantage would be only evident as the number of replications increase because the overhead of spinning up workers becomes negligible compared to workload size. As for the stability, there is a very minor difference in the rebootstrap CI values due to the built-in randomness in the function. 

### c.

```{r}
#' Extract the Coefficient of `disp` From a Linear Model
#' This function fits the linear model: \deqn{mpg \sim cyl + disp + wt} to a user-supplied data frame.
#' 
#' @param dat A data frame containing at least the variables mentioned.
#' @return A numeric scalar giving the estimated coefficient of `disp`
#'   from the fitted linear model.
#' @export

dispCoef <- function(dat) {
  # Fit the linear regression model
  fit <- lm(mpg ~ cyl + disp + wt, data = dat)

  coef(fit)[["disp"]]
}

ci2 <- makeBootstrapCI(dispCoef,
                       mtcars,
                       reps = 1000)
ci2
rebootstrap(ci2)

system.time(
  ci_serial <- makeBootstrapCI(dispCoef, mtcars, reps = 1000, compute = "serial")
)

system.time(
  ci_parallel <- makeBootstrapCI(dispCoef, mtcars, reps = 1000, compute = "parallel")
)

ci_serial
ci_parallel

rebootstrap(ci_serial)
rebootstrap(ci_parallel)
```
For timing comparison, the parallel computation is taking slightly more time to process due to overhead from setting up parallel workers. And again, the computed CIs for serial vs parallel are statistically similar but not identical, because bootstrap sampling introduces randomness.

## Problem 3

### a.

```{r}
source("artificial_data.R")

library(dplyr)
library(lme4)
library(ggplot2)
library(purrr)

df_std <- df %>%
  group_by(country) %>%
  mutate(
    prior_GPA_std    = scale(prior_gpa),
    forum_posts_std  = scale(forum_posts),
    quiz_attempts_std = scale(quiz_attempts)
  ) %>%
  ungroup()

countries <- unique(df_std$country)

results <- list()
times <- numeric(length(countries))

for (i in seq_along(countries)) {
  ctry <- countries[i]
  data_ctry <- df_std %>% filter(country == ctry)
  
  cat("Fitting model for country:", ctry, "\n")
  
  t <- system.time({
    fit <- glmer(
      completed_course ~ prior_GPA_std + forum_posts_std + quiz_attempts_std + 
        (1 | device_type),
      data = data_ctry,
      family = binomial
    )
  })
  
  results[[ctry]] <- fit
  times[i] <- t["elapsed"]
}

names(times) <- countries
times

#' Extract Forum Posts Coefficient from a Model
#'
#' This function extracts the coefficient corresponding to `forum_posts_std`
#' from a model object.
#'
#' @param model A fitted model object (`glmerMod` or `glm`) or `NULL`.
#' @return Numeric scalar: the coefficient of `forum_posts_std`, or `NA` if unavailable.
#' @export
get_forum_coef <- function(model) {
  if (is.null(model)) return(NA_real_)
  
  if (inherits(model, "glmerMod")) {
    coefs <- fixef(model)
  } else if (inherits(model, "glm")) {
    coefs <- coef(model)
  } else {
    return(NA_real_)
  }
  
  # Return numeric value only, remove name
  if ("forum_posts_std" %in% names(coefs)) {
    return(as.numeric(coefs["forum_posts_std"]))
  } else {
    return(NA_real_)
  }
}

names(results) <- countries

coef_forum <- sapply(results, get_forum_coef, USE.NAMES = TRUE)

coef_df <- data.frame(
  country = names(coef_forum),
  forum_posts_coef = coef_forum
)

coef_df

ggplot(coef_df, aes(x = country, y = forum_posts_coef)) +
  geom_col(fill = "steelblue") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Estimated Coefficient for Forum Posts by Country",
    x = "Country",
    y = "Coefficient (Standardized)"
  ) +
  theme_minimal()
```
## Problem 4

### a. How many tournaments took place in 2019

```{r}
library(data.table)

atp <- fread("atp_matches_2019.csv")

atp[, year := substr(tourney_id, 1, 4)
    ][year == "2019",
      .(tournaments_2019 = uniqueN(tourney_id))]
```

### b. Did any player win more than one tournament? If so, how many players won more than one tournament, and how many tournaments did the most winning player(s) win?

```{r}
tournament_winners <- atp[
  , .SD[which.max(match_num)],
  by = tourney_id
][
  , .(tourney_id, winner_name)
]

player_wins <- tournament_winners[
  , .(tournaments_won = .N),
  by = winner_name
][
  order(-tournaments_won)
]

players_more_than_one <- player_wins[tournaments_won > 1]
num_players_more_than_one <- nrow(players_more_than_one)
max_tournaments_won <- max(player_wins$tournaments_won)

num_players_more_than_one
max_tournaments_won
```

Yes, 17 players won more than one tournament. The most winning player won 9 tournaments, and that was Rafael Nadal.

### c. Is there any evidence that winners have more aces than losers? (If you address this with a hypothesis test, do not use base R functionality - continue to remain in the Tidyverse.)

```{r}
aces_data <- melt(
  atp,
  measure.vars = c("w_ace", "l_ace"),
  variable.name = "player_type",
  value.name = "aces"
)[
  , player_type := fifelse(player_type == "w_ace", "Winner", "Loser")
]

# 2. Summary statistics by player_type
aces_summary <- aces_data[
  , .(
      mean_aces   = mean(aces, na.rm = TRUE),
      median_aces = median(aces, na.rm = TRUE),
      sd_aces     = sd(aces, na.rm = TRUE),
      n           = .N
    ),
  by = player_type
]

aces_summary
```

Winners tend to average more aces than losers. Also here is a null hypothesis test that there is no difference in the mean number of aces between winners and losers.

```{r}
library(broom)

DT <- as.data.table(aces_data)

ace_list <- DT[
  , .(aces_list = list(aces)),
  by = player_type
]

tt <- t.test(
  ace_list[player_type == "Winner", aces_list][[1]],
  ace_list[player_type == "Loser", aces_list][[1]],
  var.equal = FALSE
)

ace_tidytest <- tidy(tt)

ace_tidytest

```

And since p \< 0.05, thereâ€™s evidence that winners tend to serve more aces.

### d. Identify the player(s) with the highest win-rate. Restrict to players with at least 5 matches.

```{r}

DT1 <- as.data.table(atp)

player_stats <- melt(
  DT1,
  measure.vars = c("winner_name", "loser_name"),
  variable.name = "result",
  value.name = "player"
)[

  , is_win := fifelse(result == "winner_name", 1L, 0L)
]

player_summary <- player_stats[
  , .(
      matches  = .N,
      wins     = sum(is_win),
      win_rate = sum(is_win) / .N
    ),
  by = player
][
  matches >= 5
][
  order(-win_rate)
]

best_players <- player_summary[win_rate == max(win_rate)]

best_players
```

Rafael Nadal is has the highest win rate, ~87%. He played 69 games.
